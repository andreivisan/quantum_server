{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df794958-3916-4ac7-99d2-a7bdee11479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be504f8b-911d-4a20-83c1-b844dcca7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('system_prompt.txt', 'r') as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e97b47e9-b8c2-4fc1-a0b2-fcfaac9c5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepResponse(BaseModel):\n",
    "    title: str = Field(description=\"title of the step\")\n",
    "    content: str = Field(description=\"content of the step\")\n",
    "    confidence: str = Field(description=\"model confidence level for current step\")\n",
    "    next_action: str = Field(description=\"next action performed by the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0044c006-edf7-4840-ac41-b00812f4b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['messages']\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=StepResponse)\n",
    "# Define the prompt template\n",
    "template = f\"\"\"{system_prompt}\n",
    "Messages: {{messages}}\n",
    "\"\"\"\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt.input_variables)\n",
    "model = OllamaLLM(model=\"llama3.2\", temperature=0)\n",
    "# Build the chain\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4512ba87-8c87-40b2-9269-8ceee4e11bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain_of_thought(user_message, max_steps=10, confidence_threshold=99):\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message),\n",
    "        AIMessage(content=\"Understood. I will now create a detailed reasoning chain following the given instructions, starting with a thorough problem decomposition.\")\n",
    "    ]\n",
    "\n",
    "    step_count = 1\n",
    "    total_thinking_time = 0\n",
    "\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            step_data = chain.invoke({\"messages\": messages})\n",
    "        except OutputParserException:\n",
    "            content = \"\"\n",
    "            step_data = {\n",
    "                \"title\": \"Raw Response\",\n",
    "                \"content\": content,\n",
    "                \"next_action\": \"continue\"\n",
    "            }\n",
    "        end_time = time.time()\n",
    "        thinking_time = end_time - start_time\n",
    "        total_thinking_time += thinking_time\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(step_data)})\n",
    "        print(\"Next reasoning step: \", step_data[\"title\"])\n",
    "\n",
    "        if step_data[\"next_action\"].lower().strip() == \"final_answer\" or step_count > 10:\n",
    "            break\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "    # Request final answer\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please provide the final answer based on your reasoning above.\",\n",
    "        })\n",
    "        try:\n",
    "            final_data = chain.invoke({\"messages\": messages})\n",
    "            break\n",
    "        except Exception:\n",
    "            print(\"Error encountered while retrieving the final answer. Retrying...\")\n",
    "            continue\n",
    "    end_time = time.time()\n",
    "    thinking_time = end_time - start_time\n",
    "    total_thinking_time += thinking_time\n",
    "\n",
    "    return final_data[\"content\"], total_thinking_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6080eae-c185-4e7c-8d3c-553a11bf7da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next reasoning step:  Problem Decomposition\n",
      "Next reasoning step:  Problem Decomposition\n",
      "Next reasoning step:  Identifying Primary Sources of Energy Consumption\n",
      "Next reasoning step:  Identifying Primary Sources of Energy Consumption\n",
      "Next reasoning step:  Identifying Primary Sources of Energy Consumption\n",
      "Next reasoning step:  Identifying Primary Sources of Energy Consumption\n",
      "Next reasoning step:  Identifying Primary Sources of Energy Consumption\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode hello world in Java\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m final_resp, total_time \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_chain_of_thought\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m, in \u001b[0;36mgenerate_chain_of_thought\u001b[0;34m(user_message, max_steps, confidence_threshold)\u001b[0m\n\u001b[1;32m     24\u001b[0m total_thinking_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m thinking_time\n\u001b[1;32m     26\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(step_data)})\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext reasoning step: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstep_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_action\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "user_message = \"code hello world in Java\"\n",
    "final_resp, total_time = generate_chain_of_thought(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971f721-9a5f-45be-bda6-877d0c975ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the steps\n",
    "print(final_resp)\n",
    "print(f\"Total Thinking Time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dfc06-a96a-4d76-b163-e95356441c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
